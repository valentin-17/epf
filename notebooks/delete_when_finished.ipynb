{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import holidays\n",
    "import IPython\n",
    "import keras_tuner as kt\n",
    "import keras\n",
    "import _pickle as pkl\n",
    "import re\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.tsa.seasonal import MSTL\n",
    "from sktime.transformations.series.outlier_detection import HampelFilter\n",
    "from sktime.transformations.series.impute import Imputer"
   ],
   "id": "457d98713b0ba08d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Prep Work",
   "id": "38f76f2f8cbeb57"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_and_concat_data(file_paths: list, column_names: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and concatenate data from multiple CSV files given by file_paths.\n",
    "\n",
    "    :param file_paths: List of file paths to the CSV files.\n",
    "    :param column_names: List of column names.\n",
    "\n",
    "    :returns: pd.DataFrame: DataFrame for each feature containing raw data from the years 2023 - 2024.\n",
    "    \"\"\"\n",
    "    df = []\n",
    "    for file_path in file_paths:\n",
    "        # Skip extra row for DK and FR prices because there is an extra disclaimer header row\n",
    "        if re.search(r'fr_prices|dk_._prices', file_path):\n",
    "            df.append(pd.read_csv(file_path, skiprows=3, names=column_names))\n",
    "        else:\n",
    "            df.append(pd.read_csv(file_path, header=1, names=column_names))\n",
    "            \n",
    "    concat_data = pd.concat(df)\n",
    "    concat_data.reset_index(drop=True, inplace=True)\n",
    "    concat_data['timestamp'] = pd.to_datetime(concat_data['timestamp'])\n",
    "    concat_data.set_index('timestamp', inplace=True)\n",
    "    \n",
    "    # if na values are present interpolate them based on the timestamp\n",
    "    if concat_data.isna().sum().sum() > 0:\n",
    "        concat_data.interpolate(method='time', inplace=True)\n",
    "    \n",
    "    return concat_data"
   ],
   "id": "28419edf1d80958a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### File Paths and Column Names config",
   "id": "bd16574ac516dc27"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "file_paths = {\n",
    "    'de_prices': ['../data/raw/de_prices_2023.csv', '../data/raw/de_prices_2024.csv'],\n",
    "    'de_load': ['../data/raw/de_load_2023.csv', '../data/raw/de_load_2024.csv'],\n",
    "    'de_solar_gen': ['../data/raw/de_solar_gen_2023.csv', '../data/raw/de_solar_gen_2024.csv'],\n",
    "    'de_wind_gen_offshore': ['../data/raw/de_wind_gen_offshore_2023.csv', '../data/raw/de_wind_gen_offshore_2024.csv'],\n",
    "    'de_wind_gen_onshore': ['../data/raw/de_wind_gen_onshore_2023.csv', '../data/raw/de_wind_gen_onshore_2024.csv'],\n",
    "    'de_gas_gen': ['../data/raw/de_gas_gen_2023.csv', '../data/raw/de_gas_gen_2024.csv'],\n",
    "    'de_lignite_gen': ['../data/raw/de_lignite_gen_2023.csv', '../data/raw/de_lignite_gen_2024.csv'],\n",
    "    'de_hard_coal_gen': ['../data/raw/de_hardcoal_gen_2023.csv', '../data/raw/de_hardcoal_gen_2024.csv'],\n",
    "    'ch_load': ['../data/raw/ch_load_2023.csv', '../data/raw/ch_load_2024.csv'],\n",
    "    'dk_load': ['../data/raw/dk_load_2023.csv', '../data/raw/dk_load_2024.csv'],\n",
    "    'fr_load': ['../data/raw/fr_load_2023.csv', '../data/raw/fr_load_2024.csv'],\n",
    "    'ch_prices': ['../data/raw/ch_prices_2023.csv', '../data/raw/ch_prices_2024.csv'],\n",
    "    'dk1_prices': ['../data/raw/dk_1_prices_2023.csv', '../data/raw/dk_1_prices_2024.csv'],\n",
    "    'dk2_prices': ['../data/raw/dk_2_prices_2023.csv', '../data/raw/dk_2_prices_2024.csv'],\n",
    "    'fr_prices': ['../data/raw/fr_prices_2023.csv', '../data/raw/fr_prices_2024.csv'],\n",
    "}\n",
    "\n",
    "column_names = {\n",
    "    'de_prices': ['timestamp', 'de_lu_price'],\n",
    "    'de_load': ['timestamp', 'de_load'],\n",
    "    'de_solar_gen': ['timestamp', 'de_solar_gen'],\n",
    "    'de_wind_gen_offshore': ['timestamp', 'de_wind_gen_offshore'],\n",
    "    'de_wind_gen_onshore': ['timestamp', 'de_wind_gen_onshore'],\n",
    "    'de_gas_gen': ['timestamp', 'de_gas_gen'],\n",
    "    'de_lignite_gen': ['timestamp', 'de_lignite_gen'],\n",
    "    'de_hard_coal_gen': ['timestamp', 'de_hard_coal_gen'],\n",
    "    'ch_load': ['timestamp', 'ch_load'],\n",
    "    'dk_load': ['timestamp', 'dk_load'],\n",
    "    'fr_load': ['timestamp', 'fr_load'],\n",
    "    'ch_prices': ['timestamp', 'ch_prices'],\n",
    "    'dk1_prices': ['timestamp', 'dk1_prices'],\n",
    "    'dk2_prices': ['timestamp', 'dk2_prices'],\n",
    "    'fr_prices': ['timestamp', 'fr_prices'],\n",
    "}"
   ],
   "id": "49ac066966d7ee69",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Populate new dataframes with raw data from years 2023 and 2024",
   "id": "d27a962917ff44f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "de_prices = load_and_concat_data(file_paths['de_prices'], column_names['de_prices'])\n",
    "de_load = load_and_concat_data(file_paths['de_load'], column_names['de_load'])\n",
    "de_solar_gen = load_and_concat_data(file_paths['de_solar_gen'], column_names['de_solar_gen'])\n",
    "de_wind_gen_offshore = load_and_concat_data(file_paths['de_wind_gen_offshore'], column_names['de_wind_gen_offshore'])\n",
    "de_wind_gen_onshore = load_and_concat_data(file_paths['de_wind_gen_onshore'], column_names['de_wind_gen_onshore'])\n",
    "de_gas_gen = load_and_concat_data(file_paths['de_gas_gen'], column_names['de_gas_gen'])\n",
    "de_lignite_gen = load_and_concat_data(file_paths['de_lignite_gen'], column_names['de_lignite_gen'])\n",
    "de_hard_coal_gen = load_and_concat_data(file_paths['de_hard_coal_gen'], column_names['de_hard_coal_gen'])\n",
    "ch_load = load_and_concat_data(file_paths['ch_load'], column_names['ch_load'])\n",
    "dk_load = load_and_concat_data(file_paths['dk_load'], column_names['dk_load'])\n",
    "fr_load = load_and_concat_data(file_paths['fr_load'], column_names['fr_load'])\n",
    "ch_prices = load_and_concat_data(file_paths['ch_prices'], column_names['ch_prices'])\n",
    "dk1_prices = load_and_concat_data(file_paths['dk1_prices'], column_names['dk1_prices'])\n",
    "dk2_prices = load_and_concat_data(file_paths['dk2_prices'], column_names['dk2_prices'])\n",
    "fr_prices = load_and_concat_data(file_paths['fr_prices'], column_names['fr_prices'])"
   ],
   "id": "ddd373be6ddf1efe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# sub-sampling the quarter hourly timeseries to hourly. see ../README.md for affected timeseries\n",
    "de_load = de_load[::4]\n",
    "de_solar_gen = de_solar_gen[::4]\n",
    "de_wind_gen_offshore = de_wind_gen_offshore[::4]\n",
    "de_wind_gen_onshore = de_wind_gen_onshore[::4]\n",
    "de_gas_gen = de_gas_gen[::4]\n",
    "de_lignite_gen = de_lignite_gen[::4]\n",
    "de_hard_coal_gen = de_hard_coal_gen[::4]"
   ],
   "id": "ea6d894d7b2bae79",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dataframes = [\n",
    "    de_prices,\n",
    "    de_load,\n",
    "    de_solar_gen,\n",
    "    de_wind_gen_offshore,\n",
    "    de_wind_gen_onshore,\n",
    "    de_gas_gen,\n",
    "    de_lignite_gen,\n",
    "    de_hard_coal_gen,\n",
    "    ch_load,\n",
    "    dk_load,\n",
    "    fr_load,\n",
    "    ch_prices,\n",
    "    dk1_prices,\n",
    "    dk2_prices,\n",
    "    fr_prices    \n",
    "]\n",
    "\n",
    "combined_df = pd.concat(dataframes, axis=1)"
   ],
   "id": "180fe04afc623ee0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# save dataset to csv in ../data/interim\n",
    "combined_df.to_csv('../data/interim/combined_data.csv', index=True)"
   ],
   "id": "a8d3c1e543fe4ce6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Statisical Analysis",
   "id": "28c967ce82893197"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# statistical overview\n",
    "combined_df.describe().transpose()"
   ],
   "id": "da1d620f5fd3fbc0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plot all timeseries for visual inspection\n",
    "plt.figure(figsize=(24, 24))\n",
    "\n",
    "for _ in combined_df.columns:\n",
    "    plt.subplot(8, 2, list(combined_df.columns).index(_)+1)\n",
    "    sns.lineplot(data=combined_df[_])\n",
    "    \n",
    "plt.savefig('../reports/figures/raw_data_overview.png', dpi=300)"
   ],
   "id": "3688cbf07d0ccfb6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    " # get pearson correlation coefficients and plot them\n",
    "correlation_matrix = combined_df.corr(method='pearson')\n",
    "correlation_matrix = correlation_matrix.drop(index=['de_lu_price'])\n",
    "correlation_matrix = correlation_matrix[['de_lu_price']].sort_values(by='de_lu_price', ascending=False)\n",
    "\n",
    "cmap = sns.diverging_palette(20, 230, as_cmap=True)\n",
    "\n",
    "plt.figure(figsize=(8, 10))\n",
    "sns.heatmap(correlation_matrix, \n",
    "            cmap=cmap,\n",
    "            annot=True,  \n",
    "            square=True, \n",
    "            cbar=False,\n",
    "            center=0,\n",
    "            xticklabels=['DE-LU Prices']).set_title('Correlation coefficients')\n",
    "\n",
    "plt.savefig('../reports/figures/de_lu_price_correlations.png', dpi=300)\n",
    "\n",
    "plt.show()"
   ],
   "id": "7ee112f4fbcd22ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### fft and autocorrelation are not suitable because given timeseries is non-stationary",
   "id": "78a718b05ce1ae9d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# to check which frequencies are present in the timeseries data fft is used\n",
    "fft = tf.signal.fft(combined_df['de_lu_price'])\n",
    "f_per_dataset = np.arange(0, len(fft))\n",
    "\n",
    "n_samples_h = len(combined_df['de_lu_price'])\n",
    "hours_per_year = 24*365.2524\n",
    "years_per_dataset = n_samples_h / hours_per_year\n",
    "\n",
    "plt.figure(figsize=(36, 8))\n",
    "f_per_year = f_per_dataset/years_per_dataset\n",
    "plt.step(f_per_year, np.abs(fft))\n",
    "plt.xscale('log')\n",
    "plt.ylim(0, 250000)\n",
    "plt.xlim([0.1, max(plt.xlim())])\n",
    "plt.xticks([1, 365.2524/7, 365.2524, 365.2524*2, 365.2524*24], labels=['1/Year', '1/7_day', '1/day', '1/12_hour', '1/hour'])\n",
    "_ = plt.xlabel('Frequency (log scale)')"
   ],
   "id": "18bc6963d5c5e077",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# to further validate time frequencies autocorrelation is measured\n",
    "plot_acf(combined_df['de_lu_price'], lags=31*24)\n",
    "plt.ylim(-0.25, 1.1)\n",
    "plt.xticks(np.arange(0, 31*24+1, 12))\n",
    "plt.show()"
   ],
   "id": "42d77a08f7840bda",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Feature Engineering",
   "id": "985865ab63422e53"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "combined_df = pd.read_csv('../data/interim/combined_data.csv', index_col=0, parse_dates=True)",
   "id": "a8678fd5e0d3276e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Detect and remove outliers",
   "id": "8cde41f12388c5af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# remove outliers from features using Hampel Filter\n",
    "def detect_and_remove_outliers(feature, window_length, n_sigma, impute_method: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Detect and remove outliers from features using Hampel Filter. Only imputes where outliers are present.\n",
    "    \n",
    "    :param feature: The feature where outliers should be removed.\n",
    "    :param window_length: Window length for Hampel Filter.\n",
    "    :param n_sigma: Number of standard deviations for outlier detection.\n",
    "    :param impute_method: Method for imputing missing values.\n",
    "    \n",
    "    :returns: DataFrame containing all features with outliers removed where applicable.\n",
    "    \"\"\"\n",
    "    hampel = HampelFilter(window_length=window_length, n_sigma=n_sigma)\n",
    "    imputer = Imputer(method=impute_method) if impute_method is not None else Imputer()\n",
    " \n",
    "    feature_hat = hampel.fit_transform(feature)\n",
    "    feature_imputed = imputer.fit_transform(feature_hat)\n",
    "        \n",
    "    return feature_imputed"
   ],
   "id": "ee68f349ff25b5c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# use nearest imputation for missing values to more or less preserve the shape of the timeseries\n",
    "WINDOW_LENGTH = 24\n",
    "N_SIGMA = 3\n",
    "METHOD = 'nearest'\n",
    "\n",
    "# detect and remove outliers in all price timeseries\n",
    "for col in combined_df.columns:\n",
    "    if 'price' in col:\n",
    "        combined_df[col + '_hat'] = detect_and_remove_outliers(combined_df[col], WINDOW_LENGTH, N_SIGMA, METHOD)\n",
    "        # drop touched cols\n",
    "        combined_df.drop(columns=[col], inplace=True)"
   ],
   "id": "94ba02124e5d984d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Remove and store seasonal component",
   "id": "5e13f156e9926a47"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# remove seasonal component from all timeseries and store seasonal component to later add back to predictions\n",
    "periods = [24, 7*24]\n",
    "seasonal_component = {}\n",
    "\n",
    "for col in combined_df.columns:\n",
    "    mstl = MSTL(combined_df[col], periods=periods).fit()\n",
    "    # remove seasonal component from timeseries by adding all seasonal components and then subtracting from original timeseries\n",
    "    combined_df[col + '_rm_seasonal'] = combined_df[col] - sum(mstl.seasonal[f'seasonal_{p}'] for p in periods)\n",
    "    seasonal_component.update({col + '_rm_seasonal': mstl})\n",
    "    # drop touched cols\n",
    "    combined_df.drop(columns=[col], inplace=True)"
   ],
   "id": "581e0ece22274cb8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# save seasonal component to pickle\n",
    "with open('../data/processed/seasonal_components.pkl', 'wb') as f:\n",
    "    pkl.dump(seasonal_component, f, -1)"
   ],
   "id": "8eee7af2c75668d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create Calendar Features",
   "id": "f064d8405e68915a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# create calendar features\n",
    "combined_df['month'] = pd.DatetimeIndex(combined_df.index).month\n",
    "# 1 = Monday, 7 = Sunday, add 1 because default is 0 = Monday\n",
    "combined_df['day_of_week'] = pd.DatetimeIndex(combined_df.index).dayofweek + 1\n",
    "# uses holiday library see references/refs.md [4]\n",
    "de_holidays = holidays.country_holidays('DE', years=[2023,2024])\n",
    "# set holiday to 1 if it is a holiday else 0\n",
    "combined_df['holiday'] = combined_df.index.to_series().apply(lambda x: 1 if x in de_holidays else 0)"
   ],
   "id": "cc767f3f678025",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create Lags",
   "id": "d07355be1665ac3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# create lagged prices according to fft analysis based off of cleaned price data de_lu_price_hat\n",
    "# 7 day, 1 day, 12 hour and 1 hour lags are introduced\n",
    "combined_df['de_lu_price_7_day_lag'] = combined_df['de_lu_price_hat_rm_seasonal'].shift(7 * 24, fill_value=0)\n",
    "combined_df['de_lu_price_1_day_lag'] = combined_df['de_lu_price_hat_rm_seasonal'].shift(24, fill_value=0)\n",
    "combined_df['de_lu_price_12_hour_lag'] = combined_df['de_lu_price_hat_rm_seasonal'].shift(12, fill_value=0)\n",
    "combined_df['de_lu_price_1_hour_lag'] = combined_df['de_lu_price_hat_rm_seasonal'].shift(1, fill_value=0)"
   ],
   "id": "c07d1a62e3b8bae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# save engineered features to csv\n",
    "combined_df_with_engineered_features = combined_df.copy()\n",
    "combined_df_with_engineered_features.to_csv('../data/interim/combined_data_with_engineered_features.csv', index=True)"
   ],
   "id": "9a4bd7d91aa05860",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "combined_df",
   "id": "69f2e37082c5d38a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create Feature set",
   "id": "efc724911751ec90"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# load combined_df with engineered features\n",
    "processed_data = pd.read_csv('../data/interim/combined_data_with_engineered_features.csv', index_col=0, parse_dates=True)"
   ],
   "id": "6fbe0a049e12dd69",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# dict like structure to dynamically select features for model training from combined_df\n",
    "features_dict = {\n",
    "    # prices\n",
    "    'de_lu_price_hat_rm_seasonal': {\n",
    "        'select': 1,\n",
    "        'name': 'DE-LU Prices',\n",
    "        'is-numerical': True\n",
    "    },\n",
    "    'ch_prices_hat_rm_seasonal': {\n",
    "        'select': 0,\n",
    "        'name': 'CH Prices',\n",
    "        'is-numerical': True\n",
    "    },\n",
    "    'dk1_prices_hat_rm_seasonal': {\n",
    "        'select': 0,\n",
    "        'name': 'DK1 Prices',\n",
    "        'is-numerical': True\n",
    "    },\n",
    "    'dk2_prices_hat_rm_seasonal': {\n",
    "        'select': 0,\n",
    "        'name': 'DK2 Prices',\n",
    "        'is-numerical': True\n",
    "    },\n",
    "    'fr_prices_hat_rm_seasonal': {\n",
    "        'select': 0,\n",
    "        'name': 'FR Prices',\n",
    "        'is-numerical': True\n",
    "    },\n",
    "    # price lags\n",
    "    'de_lu_price_7_day_lag': {\n",
    "        'select': 0,\n",
    "        'name': 'DE-LU Prices 7-Day Lag',\n",
    "        'is-numerical': True\n",
    "    },\n",
    "    'de_lu_price_1_day_lag': {\n",
    "        'select': 0,\n",
    "        'name': 'DE-LU Prices 24-Hour Lag',\n",
    "        'is-numerical': True\n",
    "    },\n",
    "    'de_lu_price_12_hour_lag': {\n",
    "        'select': 0,\n",
    "        'name': 'DE-LU Prices 12-Hour Lag',\n",
    "        'is-numerical': True\n",
    "    },\n",
    "    'de_lu_price_1_hour_lag': {\n",
    "        'select': 0,\n",
    "        'name': 'DE-LU Prices 1-Hour Lag',\n",
    "        'is-numerical': True\n",
    "    },\n",
    "    # generation\n",
    "    'de_solar_gen_rm_seasonal': {\n",
    "        'select': 1,\n",
    "        'name': 'DE Solar Generation',\n",
    "        'is-numerical': True\n",
    "    },\n",
    "    'de_wind_gen_offshore_rm_seasonal': {\n",
    "        'select': 1,\n",
    "        'name': 'DE Wind Generation Offshore',\n",
    "        'is-numerical': True\n",
    "    },\n",
    "    'de_wind_gen_onshore_rm_seasonal': {\n",
    "        'select': 1,\n",
    "        'name': 'DE Wind Generation Onshore',\n",
    "        'is-numerical': True\n",
    "    },\n",
    "    'de_gas_gen_rm_seasonal': {\n",
    "        'select': 0,\n",
    "        'name': 'DE Gas Generation',\n",
    "        'is-numerical': True\n",
    "    },\n",
    "    'de_lignite_gen_rm_seasonal': {\n",
    "        'select': 0,\n",
    "        'name': 'DE Lignite Generation',\n",
    "        'is-numerical': True\n",
    "    },\n",
    "    'de_hard_coal_gen_rm_seasonal': {\n",
    "        'select': 0,\n",
    "        'name': 'DE Hard Coal Generation',\n",
    "        'is-numerical': True\n",
    "    },\n",
    "    # loads\n",
    "    'de_load_rm_seasonal': {\n",
    "        'select': 0,\n",
    "        'name': 'DE Load',\n",
    "        'is-numerical': True\n",
    "    },\n",
    "    'ch_load_rm_seasonal': {\n",
    "        'select': 0,\n",
    "        'name': 'CH Load',\n",
    "        'is-numerical': True\n",
    "    },\n",
    "    'dk_load_rm_seasonal': {\n",
    "        'select': 0,\n",
    "        'name': 'DK Load',\n",
    "        'is-numerical': True\n",
    "    },\n",
    "    'fr_load_rm_seasonal': {\n",
    "        'select': 0,\n",
    "        'name': 'FR Load',\n",
    "        'is-numerical': True\n",
    "    },\n",
    "    # dummies\n",
    "    'month': {\n",
    "        'select': 1,\n",
    "        'name': 'Month',\n",
    "        'is-numerical': False\n",
    "    },\n",
    "    'day_of_week': {\n",
    "        'select': 1,\n",
    "        'name': 'Day of Week',\n",
    "        'is-numerical': False\n",
    "    },\n",
    "    'holiday': {\n",
    "        'select': 1,\n",
    "        'name': 'Holiday',\n",
    "        'is-numerical': False\n",
    "    },\n",
    "}"
   ],
   "id": "76a0e83f474d07f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "feature_set: pd.DataFrame = processed_data.loc[:, [k for k, v in features_dict.items() if v['select'] == 1]]",
   "id": "def239edd56e98d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# save feature set to file\n",
    "feature_set.to_csv('../data/interim/feature_set.csv', index=True)"
   ],
   "id": "b500a1f2bb910bad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model Prep",
   "id": "e2870a698e672794"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Note: largely following tutorial from [3] /references/refs.md with some additions",
   "id": "629a8349f3111887"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# load feature set from file\n",
    "feature_set = pd.read_csv('../data/interim/feature_set.csv', index_col=0, parse_dates=True)\n",
    "feature_set"
   ],
   "id": "8c805f2ea58bc668",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Sliding Window Splitting",
   "id": "16fb78452bd97e16"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Convoluted multi window splitting",
   "id": "f77012cff3ce8ecb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# train validation test splits 70, 20, 10\n",
    "# use sliding window splits to prevent overfitting\n",
    "column_indices = {name: i for i, name in enumerate(feature_set)}\n",
    "\n",
    "sliding_window_length = 24*30*3 # three month windows\n",
    "step_length = 24*7 # each window is shifted by one week\n",
    "padding = 24 # padding of one day between sliding windows to ensure no data leakage\n",
    "\n",
    "train_split = 0.7\n",
    "validation_split = 0.2\n",
    "test_split = 0.1\n",
    "\n",
    "num_splits = int((len(feature_set) - sliding_window_length) / step_length)\n",
    "num_features = feature_set.shape[1]\n",
    "\n",
    "# create python lists with num_splits entries, each entry is a sliding window\n",
    "train_df, validation_df, test_df = [], [], []\n",
    "    \n",
    "for i in range(num_splits):\n",
    "    train_df.append(feature_set.iloc[i * step_length : i * step_length + int(sliding_window_length * train_split)])\n",
    "    validation_df.append(feature_set.iloc[i * step_length + int(sliding_window_length * train_split) \n",
    "                                          : i * step_length + int(sliding_window_length * (train_split + validation_split))])\n",
    "    test_df.append(feature_set.iloc[i * step_length + int(sliding_window_length * (train_split + validation_split))\n",
    "                                    : i * step_length + int(sliding_window_length * (train_split + validation_split + test_split))])"
   ],
   "id": "af8cb52c76e2b2e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Simple single window splitting",
   "id": "68f8efce1d6a25c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def split_data(data: pd.DataFrame, train: float = 0.7, validation: float = 0.9) -> tuple:\n",
    "    \"\"\"\n",
    "    Splits data into train, validation and test sets. \n",
    "    Train and Validation params are the upper boundary for their respective splits. Lower boundaries are calculated.\n",
    "\n",
    "    :param data: DataFrame to be split.\n",
    "    :param train: Fraction of data to be used for training,\n",
    "        defaults to 0.7\n",
    "    :param validation: Fraction of data to be used for validation,\n",
    "        defaults to 0.9\n",
    "\n",
    "    :returns: Train, Validation and Test DataFrames\n",
    "    \"\"\"\n",
    "\n",
    "    train_split = int(len(data) * train)\n",
    "    validation_split = int(len(data) * validation)\n",
    "\n",
    "    return data[0:train_split], data[train_split:validation_split], data[validation_split:]"
   ],
   "id": "262f9b3a5a37a9fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Normalization",
   "id": "72ec02c37c080c3b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_df_hat, validation_df_hat, test_df_hat = [], [], []\n",
    "train_mean, train_std = [], []\n",
    "\n",
    "for i in range(num_splits):\n",
    "    # only normalize the features that are numerical and not categorical, ie exclude month, day of week and holiday from normalization\n",
    "    # Select numerical columns for normalization\n",
    "    numerical_cols = [feature_set.columns[j] for j in range(len(feature_set.columns)) if features_dict[feature_set.columns[j]]['is-numerical']]\n",
    "    categorical_cols = [col for col in feature_set.columns if col not in numerical_cols]\n",
    "\n",
    "    # Normalize numerical columns\n",
    "    train_mean.append(train_df[i][numerical_cols].mean())\n",
    "    train_std.append(train_df[i][numerical_cols].std())\n",
    "\n",
    "    train_df_hat.append((train_df[i][numerical_cols] - train_mean[i]) / train_std[i])\n",
    "    validation_df_hat.append((validation_df[i][numerical_cols] - train_mean[i]) / train_std[i])\n",
    "    test_df_hat.append((test_df[i][numerical_cols] - train_mean[i]) / train_std[i])\n",
    "\n",
    "    # Rejoin categorical columns\n",
    "    train_df_hat[i] = train_df_hat[i].join(train_df[i][categorical_cols])\n",
    "    validation_df_hat[i] = validation_df_hat[i].join(validation_df[i][categorical_cols])\n",
    "    test_df_hat[i] = test_df_hat[i].join(test_df[i][categorical_cols])"
   ],
   "id": "4fb839b08cd264c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# split and normalize\n",
    "train_df, validation_df, test_df = split_data(feature_set)"
   ],
   "id": "32ce083c3439bffa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#normalize\n",
    "train_mean = train_df.mean()\n",
    "train_std = train_df.std()\n",
    "\n",
    "train_df = (train_df - train_mean) / train_std\n",
    "validation_df = (validation_df - train_mean) / train_std\n",
    "test_df = (test_df - train_mean) / train_std\n",
    "\n",
    "num_features = feature_set.shape[1]"
   ],
   "id": "b82411ed055a025b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# exemplary plot for normalized data of 4 random sliding windows\n",
    "keys = feature_set.keys()\n",
    "\n",
    "df_std = (feature_set - train_mean) / train_std\n",
    "df_std = df_std.melt(var_name='Column', value_name='Normalized')\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.violinplot(x='Column', y='Normalized', data=df_std)\n",
    "_ = ax.set_xticklabels([features_dict[name].get('name') for name in keys], rotation=90)\n",
    "\n",
    "plt.savefig('../reports/figures/normalized_data_distribution.png', dpi=300)"
   ],
   "id": "d21f18b4667390ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# save train, val and test sets to csv\n",
    "# use the highest protocol available, denoted by -1\n",
    "with open('../data/processed/train_df.pkl', 'wb') as f:\n",
    "    pkl.dump(train_df, f, -1)\n",
    "    \n",
    "with open('../data/processed/validation_df.pkl', 'wb') as f:\n",
    "    pkl.dump(validation_df, f, -1)\n",
    "    \n",
    "with open('../data/processed/test_df.pkl', 'wb') as f:\n",
    "    pkl.dump(test_df, f, -1)"
   ],
   "id": "3d5e0a85594740b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Window generation for training",
   "id": "3201c9a13af8ed13"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# code taken from [3] /references/refs.md\n",
    "class WindowGenerator():\n",
    "    def __init__(self, input_width, label_width, shift, \n",
    "                 train_df=train_df, validation_df=validation_df, test_df=test_df, \n",
    "                 label_columns=None):\n",
    "        # Store the raw data.\n",
    "        self.train_df = train_df\n",
    "        self.validation_df = validation_df\n",
    "        self.test_df = test_df\n",
    "\n",
    "        # Work out the label column indices.\n",
    "        self.label_columns = label_columns\n",
    "        if label_columns is not None:\n",
    "            self.label_columns_indices = {name: i for i, name in enumerate(label_columns)}\n",
    "        self.column_indices = {name: i for i, name in enumerate(train_df.columns)}\n",
    "\n",
    "        # Work out the window parameters.\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "\n",
    "        self.total_window_size = input_width + shift\n",
    "\n",
    "        self.input_slice = slice(0, input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "        self.label_start = self.total_window_size - self.label_width\n",
    "        self.labels_slice = slice(self.label_start, None)\n",
    "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "            f'Total window size: {self.total_window_size}',\n",
    "            f'Input indices: {self.input_indices}',\n",
    "            f'Label indices: {self.label_indices}',\n",
    "            f'Label column name(s): {self.label_columns}'])\n",
    "    \n",
    "    def split_window(self, features):\n",
    "      inputs = features[:, self.input_slice, :]\n",
    "      labels = features[:, self.labels_slice, :]\n",
    "      if self.label_columns is not None:\n",
    "        labels = tf.stack(\n",
    "            [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "            axis=-1)\n",
    "    \n",
    "      # Slicing doesn't preserve static shape information, so set the shapes\n",
    "      # manually. This way the `tf.data.Datasets` are easier to inspect\n",
    "      inputs.set_shape([None, self.input_width, None])\n",
    "      labels.set_shape([None, self.label_width, None])\n",
    "    \n",
    "      return inputs, labels\n",
    "    \n",
    "    def make_dataset(self, data):\n",
    "      data = np.array(data, dtype=np.float32)\n",
    "      ds = keras.utils.timeseries_dataset_from_array(\n",
    "          data=data,\n",
    "          targets=None,\n",
    "          sequence_length=self.total_window_size,\n",
    "          sequence_stride=1,\n",
    "          shuffle=True,\n",
    "          batch_size=32,)\n",
    "    \n",
    "      ds = ds.map(self.split_window)\n",
    "    \n",
    "      return ds\n",
    "    \n",
    "    @property\n",
    "    def train(self):\n",
    "      return self.make_dataset(self.train_df)\n",
    "\n",
    "    @property\n",
    "    def val(self):\n",
    "      return self.make_dataset(self.validation_df)\n",
    "    \n",
    "    @property\n",
    "    def test(self):\n",
    "      return self.make_dataset(self.test_df)\n",
    "    \n",
    "    @property\n",
    "    def example(self):\n",
    "        \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
    "        result = getattr(self, '_example', None)\n",
    "        if result is None:\n",
    "          # No example batch was found, so get one from the `.train` dataset\n",
    "          result = next(iter(self.train))\n",
    "          # And cache it for next time\n",
    "          self._example = result\n",
    "        return result\n",
    "    \n",
    "    def plot(self, model=None, plot_col='de_lu_price_hat_rm_seasonal', max_subplots=3):\n",
    "      inputs, labels = self.example\n",
    "      plt.figure(figsize=(12, 8))\n",
    "      plot_col_index = self.column_indices[plot_col]\n",
    "      max_n = min(max_subplots, len(inputs))\n",
    "      for n in range(max_n):\n",
    "        plt.subplot(max_n, 1, n+1)\n",
    "        plt.ylabel(f'{plot_col} [normed]')\n",
    "        plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n",
    "                 label='Inputs', marker='.', zorder=-10)\n",
    "    \n",
    "        if self.label_columns:\n",
    "          label_col_index = self.label_columns_indices.get(plot_col, None)\n",
    "        else:\n",
    "          label_col_index = plot_col_index\n",
    "    \n",
    "        if label_col_index is None:\n",
    "          continue\n",
    "    \n",
    "        plt.scatter(self.label_indices, labels[n, :, label_col_index],\n",
    "                    edgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
    "        if model is not None:\n",
    "          predictions = model(inputs)\n",
    "          plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n",
    "                      marker='X', edgecolors='k', label='Predictions',\n",
    "                      c='#ff7f0e', s=64)\n",
    "    \n",
    "        if n == 0:\n",
    "          plt.legend()\n",
    "    \n",
    "      plt.xlabel('Time [h]')\n"
   ],
   "id": "68f4854d2bd4e0bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Multi-Step-Ahead, Single Shot Models",
   "id": "fe1ddfff8bfab9c5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# load datasets\n",
    "with open('../data/processed/train_data/train_df.pkl', 'rb') as f:\n",
    "    train_df = pkl.load(f)\n",
    "    \n",
    "with open('../data/processed/train_data/validation_df.pkl', 'rb') as f:\n",
    "    validation_df = pkl.load(f)\n",
    "    \n",
    "with open('../data/processed/train_data/test_df.pkl', 'rb') as f:\n",
    "    test_df = pkl.load(f)"
   ],
   "id": "2bc4501cc235763",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# global params\n",
    "MAX_EPOCHS = 20\n",
    "OUT_STEPS = 24\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                               patience=2,\n",
    "                                               mode='min')\n",
    "\n",
    "window = WindowGenerator(input_width=24, \n",
    "                         label_width=OUT_STEPS,\n",
    "                         shift=OUT_STEPS,\n",
    "                         label_columns=['de_prices_hat_rm_seasonal'],)\n",
    "\n",
    "val_performance = {}\n",
    "performance = {}\n",
    "\n",
    "num_features = 8\n",
    "\n",
    "window.split_window"
   ],
   "id": "b4e1c616be3d115e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Baseline",
   "id": "f5bb16500c2ac74e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class RepeatBaseline(keras.Model):\n",
    "  def call(self, inputs):\n",
    "    return inputs\n",
    "\n",
    "repeat_baseline = RepeatBaseline()\n",
    "repeat_baseline.compile(loss=keras.losses.MeanSquaredError(),\n",
    "                        metrics=[keras.metrics.MeanAbsoluteError()])\n",
    "\n",
    "IPython.display.clear_output()\n",
    "\n",
    "val_performance['Repeat'] = repeat_baseline.evaluate(window.val, return_dict=True)\n",
    "performance['Repeat'] = repeat_baseline.evaluate(window.test, verbose=0, return_dict=True)\n",
    "\n",
    "window.plot(repeat_baseline)"
   ],
   "id": "8153cb9b4b643cb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### LSTM",
   "id": "f04d69975952d0a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def lstm_builder(hp):\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # Tune the number of lstm layers choose from 32 up to 512 with steps of 32\n",
    "    hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
    "\n",
    "    # Tune the learning rate for the optimizer\n",
    "    # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    # Tune the dropout rate\n",
    "    # Choose optimal value between 0.2 and 0.5\n",
    "    hp_dropout_rate = hp.Choice('dropout_rate', values=[0.2, 0.3, 0.4, 0.5])\n",
    "\n",
    "    # build model\n",
    "    model.add(keras.layers.LSTM(hp_units, return_sequences=False))\n",
    "    model.add(keras.layers.Dense(OUT_STEPS*num_features,\n",
    "                              kernel_initializer=keras.initializers.zeros()))\n",
    "    model.add(keras.layers.Reshape([OUT_STEPS, num_features]))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(loss=keras.losses.MeanSquaredError(),\n",
    "                  optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                  metrics=[keras.metrics.MeanAbsoluteError()])\n",
    "\n",
    "    return model"
   ],
   "id": "51bb78e9f4e8ef62",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lstm_tuner = (kt.BayesianOptimization\n",
    "(\n",
    "    lstm_builder,\n",
    "    objective='mean_absolute_error',\n",
    "    max_trials=10,\n",
    "    directory='../models/tuner',\n",
    "))\n",
    "\n",
    "stop_early = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "lstm_tuner.search(\n",
    "    window.train,\n",
    "    epochs=MAX_EPOCHS,\n",
    "    validation_data=window.val,\n",
    "    callbacks=[stop_early]\n",
    ")\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = lstm_tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_lstm_model = lstm_tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
    "is {best_hps.get('learning_rate')}.\n",
    "\"\"\")"
   ],
   "id": "b47bcd9188365bed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "history_lstm = best_lstm_model.fit(window.train, epochs=MAX_EPOCHS,\n",
    "                                   validation_data=window.val,\n",
    "                                   callbacks=[early_stopping])\n",
    "\n",
    "IPython.display.clear_output()\n",
    "\n",
    "val_performance['LSTM'] = best_lstm_model.evaluate(window.val, return_dict=True)\n",
    "performance['LSTM'] = best_lstm_model.evaluate(window.test, verbose=0, return_dict=True)\n",
    "\n",
    "best_lstm_model.save('../models/lstm_vre_only.keras')"
   ],
   "id": "54cdf5785aba1063",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### GRU",
   "id": "908abe35d7abb105"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def gru_builder(hp):\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # Tune the number of lstm layers choose from 32 up to 512 with steps of 32\n",
    "    hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
    "      \n",
    "    # Tune the learning rate for the optimizer\n",
    "    # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    \n",
    "    # build model\n",
    "    model.add(keras.layers.LSTM(hp_units, return_sequences=False))\n",
    "    model.add(keras.layers.Dense(OUT_STEPS*num_features,\n",
    "                              kernel_initializer=keras.initializers.zeros()))\n",
    "    model.add(keras.layers.Reshape([OUT_STEPS, num_features]))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(loss=keras.losses.MeanSquaredError(),\n",
    "                  optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                  metrics=[keras.metrics.MeanAbsoluteError()])\n",
    "    \n",
    "    return model"
   ],
   "id": "76973d3fa66f6d48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "gru_tuner = (kt.BayesianOptimization\n",
    "(\n",
    "    gru_builder,\n",
    "    objective='mean_absolute_error',\n",
    "    max_trials=10,\n",
    "    directory='../models/tuner',\n",
    "    overwrite=True,\n",
    "))\n",
    "\n",
    "stop_early = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "gru_tuner.search(\n",
    "    window.train,\n",
    "    epochs=MAX_EPOCHS,\n",
    "    validation_data=window.val,\n",
    "    callbacks=[stop_early]\n",
    ")\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = gru_tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_gru_model = gru_tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
    "is {best_hps.get('learning_rate')}.\n",
    "\"\"\")"
   ],
   "id": "1035a992807d824",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "history_gru = best_gru_model.fit(window.train, epochs=MAX_EPOCHS,\n",
    "                                 validation_data=window.val,\n",
    "                                 callbacks=[early_stopping])\n",
    "\n",
    "IPython.display.clear_output()\n",
    "\n",
    "val_performance['GRU'] = best_gru_model.evaluate(window.val, return_dict=True)\n",
    "performance['GRU'] = best_gru_model.evaluate(window.test, verbose=0, return_dict=True)\n",
    "\n",
    "#best_gru_model.save('../models/gru_vre_only.keras')"
   ],
   "id": "977becb12fafe5e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### CNN",
   "id": "4bff68a09884324a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def conv_builder(hp):\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # Tune the number of lstm layers choose from 32 up to 512 with steps of 32\n",
    "    hp_units = hp.Int('units', min_value=32, max_value=2048, step=32)\n",
    "      \n",
    "    # Tune the learning rate for the optimizer\n",
    "    # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    \n",
    "    # tune the kernel size for the convolutional layer choose from 1 to 10\n",
    "    hp_kernel_size = hp.Int('kernel_size', min_value=1, max_value=10, step=1)\n",
    "    \n",
    "    # build model\n",
    "    model.add(keras.layers.Lambda(lambda x: x[:, -hp_kernel_size:, :]))\n",
    "    model.add(keras.layers.Conv1D(hp_units, activation='relu', kernel_size=hp_kernel_size))\n",
    "    model.add(keras.layers.Dense(OUT_STEPS*num_features,\n",
    "                                 kernel_initializer=keras.initializers.zeros()))\n",
    "    model.add(keras.layers.Reshape([OUT_STEPS, num_features]))\n",
    "    \n",
    "    # compile the model\n",
    "    model.compile(loss=keras.losses.MeanSquaredError(),\n",
    "                  optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                  metrics=[keras.metrics.MeanAbsoluteError()])\n",
    "    \n",
    "    return model"
   ],
   "id": "25f5eb389e35c64b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "conv_tuner = (kt.BayesianOptimization\n",
    "(\n",
    "    conv_builder,\n",
    "    objective='mean_absolute_error',\n",
    "    max_trials=10,\n",
    "    directory='../models/tuner',\n",
    "    overwrite=True,\n",
    "))\n",
    "\n",
    "stop_early = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "conv_tuner.search(\n",
    "    window.train,\n",
    "    epochs=MAX_EPOCHS,\n",
    "    validation_data=window.val,\n",
    "    callbacks=[stop_early]\n",
    ")\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = conv_tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_conv_model = conv_tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
    "is {best_hps.get('learning_rate')}.\n",
    "\"\"\")"
   ],
   "id": "d1c0b720f5935897",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "history_conv = best_conv_model.fit(window.train, epochs=MAX_EPOCHS,\n",
    "                                   validation_data=window.val,\n",
    "                                   callbacks=[early_stopping])\n",
    "\n",
    "IPython.display.clear_output()\n",
    "\n",
    "val_performance['Conv'] = best_conv_model.evaluate(window.val, return_dict=True)\n",
    "performance['Conv'] = best_conv_model.evaluate(window.test, verbose=0, return_dict=True)\n",
    "\n",
    "#best_conv_model.save('../models/conv_vre_only.keras')"
   ],
   "id": "dc720285ce0bc159",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Evaluation",
   "id": "5c511d233ca44bc8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "performance",
   "id": "4c8639197d19833d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x = np.arange(len(performance))\n",
    "width = 0.3\n",
    "\n",
    "metric_name = 'mean_absolute_error'\n",
    "val_mae = [v[metric_name] for v in val_performance.values()]\n",
    "test_mae = [v[metric_name] for v in performance.values()]\n",
    "\n",
    "plt.bar(x - 0.17, val_mae, width, label='Validation')\n",
    "plt.bar(x + 0.17, test_mae, width, label='Test')\n",
    "plt.xticks(ticks=x, labels=performance.keys(),\n",
    "           rotation=45)\n",
    "plt.ylabel(f'MAE (average over all times and outputs)')\n",
    "_ = plt.legend()\n"
   ],
   "id": "2c11b4cc43a685f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for name, value in performance.items():\n",
    "  print(f'{name:7s}: {value[metric_name]:0.4f}')"
   ],
   "id": "d199dc9b3de2a245",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "window.plot(best_lstm_model, 'de_lu_price_hat_rm_seasonal')",
   "id": "dd1987becb295b70",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "window.plot(best_gru_model, 'de_lu_price_hat_rm_seasonal')",
   "id": "4ddb4c315ad6b2b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "window.plot(best_conv_model, 'de_lu_price_hat_rm_seasonal')",
   "id": "43ddc70831fbf88c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
